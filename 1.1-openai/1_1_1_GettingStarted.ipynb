{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvCwh4h184toGv1EsFUuCi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vchandraiitk/PokerCardGame/blob/master/1.1-openai/1_1_1_GettingStarted.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Getting started With Langchain And Open AI\n",
        "\n",
        "In this quickstart we'll see how to:\n",
        "\n",
        "- Get setup with LangChain, LangSmith and LangServe\n",
        "- Use the most basic and common components of LangChain: prompt templates, models, and output parsers.\n",
        "- Build a simple application with LangChain\n",
        "- Trace your application with LangSmith\n",
        "- Serve your application with LangServe"
      ],
      "metadata": {
        "id": "TojCjjcXXlIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip --quiet install dotenv langchain_openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8VdL8TgXy4H",
        "outputId": "fb8ee674-0343-49a6-d2ca-689c00fa402f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.4/63.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/438.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m430.1/438.5 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.5/438.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NrzGrHsnXcZC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from google.colab import userdata\n",
        "load_dotenv()\n",
        "\n",
        "os.environ['OPENAI_API_KEY']=userdata.get(\"OPENAI_API_KEY\")\n",
        "## Langsmith Tracking\n",
        "os.environ[\"LANGCHAIN_API_KEY\"]=userdata.get(\"LANGCHAIN_API_KEY\")\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"]=userdata.get(\"LANGCHAIN_PROJECT\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "llm=ChatOpenAI(model=\"gpt-4o\")\n",
        "print(llm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7x71EkvGZdn5",
        "outputId": "6ac28277-7464-4f96-df18-1055b9f605ec"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "client=<openai.resources.chat.completions.completions.Completions object at 0x7f87ac217ad0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7f87a7a63b10> root_client=<openai.OpenAI object at 0x7f87ac7f96d0> root_async_client=<openai.AsyncOpenAI object at 0x7f87a7dc1350> model_name='gpt-4o' model_kwargs={} openai_api_key=SecretStr('**********')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Input and get response form LLM\n",
        "\n",
        "result=llm.invoke(\"What is generative AI?\")"
      ],
      "metadata": {
        "id": "OMgNmRDOZrUS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTTHY-hSZuXD",
        "outputId": "941709f2-ed8d-4839-a5dc-ea2b984edd77"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content=\"Generative AI refers to a category of artificial intelligence models designed to generate new content. These models can create text, images, music, and other media based on the data they have been trained on. They utilize techniques from machine learning, particularly deep learning, to learn patterns and structures from existing data and then produce novel outputs that mimic these patterns.\\n\\nOne well-known type of generative AI is the Generative Adversarial Network (GAN), which consists of two neural networks: a generator and a discriminator. The generator creates new data instances, while the discriminator evaluates them for authenticity. Through this adversarial process, the generator learns to produce increasingly realistic content.\\n\\nAnother prominent approach is the use of transformer-based models, such as OpenAI's GPT (Generative Pre-trained Transformer), which have shown remarkable capabilities in generating human-like text and performing various language tasks.\\n\\nApplications of generative AI span numerous fields, including:\\n\\n1. **Text Generation:** Crafting articles, stories, poetry, or dialogues.\\n2. **Image Creation:** Producing artworks, synthetic photos, or enhancing existing images.\\n3. **Music Composition:** Composing new music tracks or generating soundscapes.\\n4. **Data Augmentation:** Generating synthetic data to improve machine learning model training.\\n5. **Design and Creativity:** Assisting in fashion design, architecture, and other creative endeavors.\\n\\nGenerative AI holds significant potential to automate creative processes, enhance productivity, and support innovation across various industries. However, it also raises ethical and societal questions, particularly concerning authenticity, intellectual property, and the potential misuse of generated content.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 318, 'prompt_tokens': 13, 'total_tokens': 331, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a288987b44', 'id': 'chatcmpl-BdXRqmeeP267YYiC20BUjmPizcaz4', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--c3a1200a-ee61-44ae-998d-b792b0048fa6-0' usage_metadata={'input_tokens': 13, 'output_tokens': 318, 'total_tokens': 331, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Chatprompt Template\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt=ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\",\"You are an expert AI Engineer. Provide me answers based on the questions\"),\n",
        "        (\"user\",\"{input}\")\n",
        "    ]\n",
        "\n",
        ")\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKcd9pXBZv4w",
        "outputId": "386aba55-011b-465d-8de1-e7ae11526b98"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answers based on the questions'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## chain\n",
        "chain=prompt|llm\n",
        "\n",
        "response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCPq6NChaAxb",
        "outputId": "d8a66b7f-64b9-46b6-95b9-136744c01514"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Langsmith is a tool designed to help developers build language model applications more efficiently and effectively. It offers features that allow users to better evaluate, test, and monitor language models and chains in their applications. With Langsmith, developers can log and visualize traces of LLM calls, inspect data generations, and analyze the performance and output of their applications. This helps in debugging, improving the robustness of the app, and ensuring that it aligns well with user expectations. Langsmith is particularly useful in scenarios where complex language model pipelines are built and extensive evaluation is necessary to fine-tune and optimize model interactions.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 119, 'prompt_tokens': 33, 'total_tokens': 152, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a288987b44', 'id': 'chatcmpl-BdXTFtyhOvIEYLygGxLvsGYhQBYhQ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--7402fbf1-b6a5-49bb-9d27-8f929accc5a4-0' usage_metadata={'input_tokens': 33, 'output_tokens': 119, 'total_tokens': 152, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "id": "irOq1-ljaD_x",
        "outputId": "21c04094-3a74-4780-b5b6-19fbb437c7ff"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain_core.messages.ai.AIMessage"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.messages.ai.AIMessage</b><br/>def __init__(content: Union[str, list[Union[str, dict]]], **kwargs: Any) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/langchain_core/messages/ai.py</a>Message from an AI.\n",
              "\n",
              "AIMessage is returned from a chat model as a response to a prompt.\n",
              "\n",
              "This message represents the output of the model and consists of both\n",
              "the raw output as returned by the model together standardized fields\n",
              "(e.g., tool calls, usage metadata) added by the LangChain framework.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 149);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## stroutput Parser\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "output_parser=StrOutputParser()\n",
        "chain=prompt|llm|output_parser\n",
        "\n",
        "response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSlA_Y-OaFtP",
        "outputId": "7765c28f-3b4d-4f08-f879-89087450d48f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Langsmith is a tool designed to enhance the development, testing, and monitoring of applications built with large language models (LLMs). It provides a robust platform for monitoring and evaluating the performance of language model applications, particularly those built using LangChain, a popular framework for building applications with LLMs. Langsmith allows developers to trace interactions, assess the efficacy of chains and agents, and provides an interface to log output, enabling better iteration and debugging of language model applications. The tool is essential for understanding how LLMs perform in real-world scenarios and offers features like prompt management, versioning, and performance analytics to streamline the development process.\n"
          ]
        }
      ]
    }
  ]
}